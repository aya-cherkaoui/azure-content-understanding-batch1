{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f280d612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# âš ï¸ Ne pas committer de clÃ© API en dur â€” utiliser une variable d'environnement\n",
    "client = AzureOpenAI(\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"https://content-understanding--resource.cognitiveservices.azure.com/\"),\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\", \"YOUR_API_KEY_HERE\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb908db2",
   "metadata": {},
   "source": [
    "# ğŸ“„ Azure Content Understanding â€” Prebuilt Analyzer Test\n",
    "\n",
    "**Service** : Azure AI Content Understanding (REST API `2025-11-01`)  \n",
    "**Auth** : Azure AD (DefaultAzureCredential) â€” key auth disabled on this resource  \n",
    "**Plan** :  \n",
    "1. Acquire token once (cached)  \n",
    "2. Test available prebuilt analyzers on 1 doc  \n",
    "3. Run the best ones on 20 docs from `batch1_1`  \n",
    "4. Save JSON results to `docu_results_batch1_1/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaf1cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests\n",
    "%pip install cryptography --only-binary=:all:\n",
    "%pip install azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8c887d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”‘ Token OK\n",
      "ğŸŒ Endpoint: https://aya-demo-ai.cognitiveservices.azure.com\n",
      "ğŸ“‚ 20 documents ready\n"
     ]
    }
   ],
   "source": [
    "import os, json, time, glob, base64, requests\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# â”€â”€ Config â”€â”€\n",
    "ENDPOINT    = \"https://aya-demo-ai.cognitiveservices.azure.com\"\n",
    "API_VERSION = \"2025-11-01\"\n",
    "DOCS_FOLDER  = r\"c:\\Users\\ayac\\Downloads\\archive (1)\\batch_1\\batch_1\\batch1_1\"\n",
    "OUTPUT_FOLDER = r\"c:\\Users\\ayac\\Downloads\\archive (1)\\batch_1\\batch_1\\docu_results_batch1_1\"\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# â”€â”€ Azure AD Token (cached) â”€â”€\n",
    "credential = DefaultAzureCredential()\n",
    "_token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "def auth():\n",
    "    global _token\n",
    "    if time.time() > _token.expires_on - 120:\n",
    "        _token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "    return {\"Authorization\": f\"Bearer {_token.token}\"}\n",
    "\n",
    "# â”€â”€ First 20 images â”€â”€\n",
    "all_images = sorted(glob.glob(os.path.join(DOCS_FOLDER, \"*.jpg\")))[:20]\n",
    "print(f\"ğŸ”‘ Token OK\")\n",
    "print(f\"ğŸŒ Endpoint: {ENDPOINT}\")\n",
    "print(f\"ğŸ“‚ {len(all_images)} documents ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1b1fa5",
   "metadata": {},
   "source": [
    "## ğŸ§ª Step 1 â€” Test which prebuilt analyzers work on this resource\n",
    "\n",
    "Quick test on 1 document to identify working analyzers before running the full batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b45962a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing prebuilt-invoice on batch1-0001.jpg...\n",
      "   (first call may take 3-6 min while models deploy)\n",
      "\n",
      "   âœ“ Submitted (202). Polling (10 min timeout)...\n",
      "\n",
      "   âœ… prebuilt-invoice SUCCESS! (8s)\n",
      "      Fields: 31 | Markdown: 13 chars\n",
      "      Field names: ['AmountDue', 'BalanceForward', 'BillingAddress', 'BillingAddressRecipient', 'CountryRegion', 'CustomerAddress', 'CustomerAddressRecipient', 'CustomerId', 'CustomerName', 'CustomerTaxId']\n",
      "\n",
      "ğŸ“Œ Analyzers for batch: ['prebuilt-invoice']\n"
     ]
    }
   ],
   "source": [
    "def submit(image_path, analyzer_id):\n",
    "    \"\"\"POST image to Content Understanding â†’ returns operation URL.\"\"\"\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        data = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    url = f\"{ENDPOINT}/contentunderstanding/analyzers/{analyzer_id}:analyze?api-version={API_VERSION}\"\n",
    "    r = requests.post(url, headers={**auth(), \"Content-Type\": \"application/json\"},\n",
    "                      json={\"inputs\": [{\"data\": data, \"mimeType\": \"image/jpeg\"}]})\n",
    "    if r.status_code != 202:\n",
    "        raise RuntimeError(f\"{r.status_code}: {r.text[:500]}\")\n",
    "    return r.headers[\"Operation-Location\"]\n",
    "\n",
    "def poll(op_url, timeout=600):\n",
    "    \"\"\"Poll operation URL until Succeeded (default 10 min timeout).\"\"\"\n",
    "    for i in range(timeout // 5):\n",
    "        time.sleep(5)\n",
    "        r = requests.get(op_url, headers=auth())\n",
    "        r.raise_for_status()\n",
    "        res = r.json()\n",
    "        st = res.get(\"status\", \"\")\n",
    "        if st == \"Succeeded\": return res\n",
    "        if st in (\"Failed\",\"Canceled\"):\n",
    "            raise RuntimeError(json.dumps(res.get(\"error\", res), indent=2))\n",
    "        if i % 6 == 5:\n",
    "            print(f\"    ...still waiting ({(i+1)*5}s, status={st})\", flush=True)\n",
    "    raise TimeoutError(\"Timeout\")\n",
    "\n",
    "# â”€â”€ Test just prebuilt-invoice first (may take time on first call) â”€â”€\n",
    "test_img = all_images[0]\n",
    "print(f\"ğŸ§ª Testing prebuilt-invoice on {os.path.basename(test_img)}...\")\n",
    "print(f\"   (first call may take 3-6 min while models deploy)\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "try:\n",
    "    op = submit(test_img, \"prebuilt-invoice\")\n",
    "    print(f\"   âœ“ Submitted (202). Polling (10 min timeout)...\")\n",
    "    res = poll(op, timeout=600)\n",
    "    dt = time.time() - t0\n",
    "    c = res.get(\"result\",{}).get(\"contents\",[])\n",
    "    block = c[0] if c else {}\n",
    "    fields = block.get(\"fields\",{})\n",
    "    md = block.get(\"markdown\",\"\")\n",
    "    print(f\"\\n   âœ… prebuilt-invoice SUCCESS! ({dt:.0f}s)\")\n",
    "    print(f\"      Fields: {len(fields)} | Markdown: {len(md)} chars\")\n",
    "    print(f\"      Field names: {list(fields.keys())[:10]}\")\n",
    "    ANALYZERS = [\"prebuilt-invoice\"]\n",
    "except Exception as e:\n",
    "    dt = time.time() - t0\n",
    "    print(f\"\\n   âŒ prebuilt-invoice failed ({dt:.0f}s): {e}\")\n",
    "    ANALYZERS = [\"prebuilt-layout\", \"prebuilt-read\"]\n",
    "    print(f\"   Falling back to: {ANALYZERS}\")\n",
    "\n",
    "print(f\"\\nğŸ“Œ Analyzers for batch: {ANALYZERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c87ba069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Checking if previous prebuilt-invoice job completed...\n",
      "\n",
      "   Submitted. Polling with 10 min timeout...\n",
      "\n",
      "âœ… prebuilt-invoice works! (7s)\n",
      "   Fields: 31 | Markdown: 13 chars\n",
      "   Field names: ['AmountDue', 'BalanceForward', 'BillingAddress', 'BillingAddressRecipient', 'CountryRegion', 'CustomerAddress', 'CustomerAddressRecipient', 'CustomerId', 'CustomerName', 'CustomerTaxId', 'DueDate', 'InvoiceDate', 'InvoiceId', 'LineItems', 'PaymentTerm', 'PONumber', 'RemittanceAddress', 'RemittanceAddressRecipient', 'ServiceAddress', 'ServiceAddressRecipient', 'ShippingAddress', 'ShippingAddressRecipient', 'SubtotalAmount', 'TaxDetails', 'TotalAmount', 'TotalDiscountAmount', 'TotalTaxAmount', 'VendorAddress', 'VendorAddressRecipient', 'VendorName', 'VendorTaxId']\n",
      "\n",
      "ğŸ“Œ Will use: ['prebuilt-invoice'] for the batch!\n"
     ]
    }
   ],
   "source": [
    "# Check if the previous prebuilt-invoice job finished by now\n",
    "# (it was submitted ~5 min ago, may have completed since)\n",
    "print(\"ğŸ” Checking if previous prebuilt-invoice job completed...\\n\")\n",
    "\n",
    "# Re-submit a fresh one and poll with very long timeout\n",
    "t0 = time.time()\n",
    "op = submit(all_images[0], \"prebuilt-invoice\")\n",
    "print(f\"   Submitted. Polling with 10 min timeout...\")\n",
    "\n",
    "try:\n",
    "    res = poll(op, timeout=600)\n",
    "    dt = time.time() - t0\n",
    "    c = res.get(\"result\",{}).get(\"contents\",[])\n",
    "    block = c[0] if c else {}\n",
    "    fields = block.get(\"fields\",{})\n",
    "    md = block.get(\"markdown\",\"\")\n",
    "    print(f\"\\nâœ… prebuilt-invoice works! ({dt:.0f}s)\")\n",
    "    print(f\"   Fields: {len(fields)} | Markdown: {len(md)} chars\")\n",
    "    print(f\"   Field names: {list(fields.keys())}\")\n",
    "    \n",
    "    # Save this test result\n",
    "    os.makedirs(os.path.join(OUTPUT_FOLDER, \"prebuilt-invoice\"), exist_ok=True)\n",
    "    with open(os.path.join(OUTPUT_FOLDER, \"prebuilt-invoice\", \"batch1-0001.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(res, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    ANALYZERS = [\"prebuilt-invoice\"]\n",
    "    print(f\"\\nğŸ“Œ Will use: {ANALYZERS} for the batch!\")\n",
    "except Exception as e:\n",
    "    dt = time.time() - t0\n",
    "    print(f\"\\nâŒ Still failing ({dt:.0f}s): {e}\")\n",
    "    print(\"   The GPT models may still be deploying on your resource.\")\n",
    "    print(\"   You can check deployment status in Azure Portal â†’ aya-demo-ai â†’ Model deployments\")\n",
    "    ANALYZERS = [\"prebuilt-layout\", \"prebuilt-read\"]\n",
    "    print(f\"\\nğŸ“Œ Falling back to: {ANALYZERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b43d969",
   "metadata": {},
   "source": [
    "## ğŸš€ Step 2 â€” Batch analysis: submit ALL, then poll ALL\n",
    "\n",
    "Submits all requests first (fast), then collects results â€” much faster than sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6bfe37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¤ Submitting all requests...\n",
      "  âœ“ [1/20] batch1-0001.jpg â†’ prebuilt-invoice\n",
      "  âœ“ [2/20] batch1-0002.jpg â†’ prebuilt-invoice\n",
      "  âœ“ [3/20] batch1-0003.jpg â†’ prebuilt-invoice\n",
      "  âœ“ [4/20] batch1-0004.jpg â†’ prebuilt-invoice\n",
      "  âœ“ [5/20] batch1-0005.jpg â†’ prebuilt-invoice\n",
      "  âœ“ [6/20] batch1-0006.jpg â†’ prebuilt-invoice\n",
      "  âœ“ [7/20] batch1-0007.jpg â†’ prebuilt-invoice\n",
      "  âœ“ [8/20] batch1-0008.jpg â†’ prebuilt-invoice\n",
      "  âœ“ [9/20] batch1-0009.jpg â†’ prebuilt-invoice\n",
      "  âœ“ [10/20] batch1-0010.jpg â†’ prebuilt-invoice\n",
      "  âœ“ [11/20] batch1-0011.jpg â†’ prebuilt-invoice\n",
      "  âœ“ [12/20] batch1-0012.jpg â†’ prebuilt-invoice\n",
      "  âœ“ [13/20] batch1-0013.jpg â†’ prebuilt-invoice\n",
      "  âœ“ [14/20] batch1-0014.jpg â†’ prebuilt-invoice\n",
      "  âœ“ [15/20] batch1-0015.jpg â†’ prebuilt-invoice\n",
      "  âœ“ [16/20] batch1-0016.jpg â†’ prebuilt-invoice\n",
      "  âœ“ [17/20] batch1-0017.jpg â†’ prebuilt-invoice\n",
      "  âœ“ [18/20] batch1-0018.jpg â†’ prebuilt-invoice\n",
      "  âœ“ [19/20] batch1-0019.jpg â†’ prebuilt-invoice\n",
      "  âœ“ [20/20] batch1-0020.jpg â†’ prebuilt-invoice\n",
      "\n",
      "ğŸ“¤ 20 jobs submitted. Waiting 8s before polling...\n",
      "\n",
      "ğŸ“¥ Collecting results...\n",
      "    ...still waiting (30s, status=Running)\n",
      "    ...still waiting (60s, status=Running)\n",
      "    ...still waiting (90s, status=Running)\n",
      "    ...still waiting (120s, status=Running)\n",
      "    ...still waiting (150s, status=Running)\n",
      "  âœ… [1/20] batch1-0001.jpg|prebuilt-invoice 218s f=31 md=13\n",
      "     ğŸ“‹ Fields: {'AmountDue': '[dict]', 'BalanceForward': '[dict]', 'BillingAddress': \"{'type': 'string'}\", 'BillingAddressRecipient': \"{'type': 'string'}\", 'CountryRegion': \"{'type': 'string'}\"}\n",
      "     ğŸ“ Le contenu extrait du document \"batch1-0001.jpg\" est vide ; aucune information nâ€™a Ã©tÃ© dÃ©tectÃ©e dans le texte fourni. Il\n",
      "  âœ… [2/20] batch1-0002.jpg|prebuilt-invoice 225s f=31 md=13\n",
      "     ğŸ“‹ Fields: {'AmountDue': '[dict]', 'BalanceForward': '[dict]', 'BillingAddress': \"{'type': 'string'}\", 'BillingAddressRecipient': \"{'type': 'string'}\", 'CountryRegion': \"{'type': 'string'}\"}\n",
      "     ğŸ“ Le contenu extrait du document \"batch1-0002.jpg\" est vide ; aucune information nâ€™a Ã©tÃ© dÃ©tectÃ©e. Il nâ€™est donc pas possi\n",
      "  âœ… [3/20] batch1-0003.jpg|prebuilt-invoice 231s f=31 md=13\n",
      "     ğŸ“‹ Fields: {'AmountDue': '[dict]', 'BalanceForward': '[dict]', 'BillingAddress': \"{'type': 'string'}\", 'BillingAddressRecipient': \"{'type': 'string'}\", 'CountryRegion': \"{'type': 'string'}\"}\n",
      "     ğŸ“ Le contenu extrait du document \"batch1-0003.jpg\" est vide, il n'y a donc aucune information disponible permettant d'iden\n",
      "  âœ… [4/20] batch1-0004.jpg|prebuilt-invoice 238s f=31 md=13\n",
      "     ğŸ“‹ Fields: {'AmountDue': '[dict]', 'BalanceForward': '[dict]', 'BillingAddress': \"{'type': 'string'}\", 'BillingAddressRecipient': \"{'type': 'string'}\", 'CountryRegion': \"{'type': 'string'}\"}\n",
      "     ğŸ“ Le contenu extrait du document \"batch1-0004.jpg\" est vide, il n'y a donc aucune information disponible permettant d'iden\n",
      "  âœ… [5/20] batch1-0005.jpg|prebuilt-invoice 245s f=31 md=13\n",
      "     ğŸ“‹ Fields: {'AmountDue': '[dict]', 'BalanceForward': '[dict]', 'BillingAddress': \"{'type': 'string'}\", 'BillingAddressRecipient': \"{'type': 'string'}\", 'CountryRegion': \"{'type': 'string'}\"}\n",
      "     ğŸ“ Le contenu extrait du document \"batch1-0005.jpg\" est vide ; aucune information nâ€™a Ã©tÃ© dÃ©tectÃ©e. Il nâ€™est donc pas possi\n",
      "    ...still waiting (30s, status=Running)\n",
      "    ...still waiting (60s, status=Running)\n",
      "    ...still waiting (90s, status=Running)\n",
      "    ...still waiting (120s, status=Running)\n",
      "    ...still waiting (150s, status=Running)\n",
      "    ...still waiting (180s, status=Running)\n",
      "    ...still waiting (210s, status=Running)\n"
     ]
    }
   ],
   "source": [
    "all_metrics = {a: [] for a in ANALYZERS}\n",
    "jobs = []  # (img_path, analyzer, op_url, t0)\n",
    "\n",
    "# â”€â”€ Helper: extract field values (name â†’ value) recursively â”€â”€\n",
    "def extract_field_values(fields_dict):\n",
    "    \"\"\"Flatten fields into {name: value} dict, handling nested/array fields.\"\"\"\n",
    "    result = {}\n",
    "    for name, obj in fields_dict.items():\n",
    "        if isinstance(obj, dict):\n",
    "            val = obj.get(\"valueString\") or obj.get(\"valueNumber\") or obj.get(\"valueDate\") or obj.get(\"content\") or obj.get(\"value\")\n",
    "            if val is not None:\n",
    "                result[name] = val\n",
    "            # Handle sub-fields (e.g. VendorAddress)\n",
    "            elif \"valueObject\" in obj:\n",
    "                sub = extract_field_values(obj[\"valueObject\"])\n",
    "                result[name] = sub\n",
    "            # Handle arrays (e.g. Items)\n",
    "            elif \"valueArray\" in obj:\n",
    "                arr = []\n",
    "                for item in obj[\"valueArray\"]:\n",
    "                    if isinstance(item, dict) and \"valueObject\" in item:\n",
    "                        arr.append(extract_field_values(item[\"valueObject\"]))\n",
    "                    elif isinstance(item, dict):\n",
    "                        v = item.get(\"valueString\") or item.get(\"content\") or item.get(\"value\")\n",
    "                        arr.append(v)\n",
    "                    else:\n",
    "                        arr.append(item)\n",
    "                result[name] = arr\n",
    "            else:\n",
    "                result[name] = str(obj)[:200]  # fallback\n",
    "    return result\n",
    "\n",
    "# â”€â”€ Helper: LLM description via GPT deployed on Cognitive Services â”€â”€\n",
    "GPT_DEPLOYMENT = \"gpt-4.1\"  # nom du dÃ©ploiement GPT sur la ressource aya-demo-ai\n",
    "\n",
    "def llm_describe(markdown_text, fname):\n",
    "    \"\"\"Call GPT deployed on the Cognitive Services resource to get a global document description.\"\"\"\n",
    "    prompt = f\"\"\"Tu es un assistant d'analyse documentaire. Voici le contenu extrait d'un document scannÃ© nommÃ© \"{fname}\".\n",
    "\n",
    "Contenu du document:\n",
    "{markdown_text[:1500]}\n",
    "\n",
    "Donne une description globale de ce document en 2-3 phrases en franÃ§ais: quel type de document est-ce (facture, devis, bon de commande...), qui est l'Ã©metteur, le destinataire, le montant total, la date, et tout Ã©lÃ©ment clÃ©.\"\"\"\n",
    "\n",
    "    url = f\"{ENDPOINT}/openai/deployments/{GPT_DEPLOYMENT}/chat/completions?api-version=2024-12-01-preview\"\n",
    "    body = {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": 200,\n",
    "        \"temperature\": 0.3,\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(url, headers={**auth(), \"Content-Type\": \"application/json\"}, json=body)\n",
    "        r.raise_for_status()\n",
    "        return r.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        return f\"[LLM error: {e}]\"\n",
    "\n",
    "# â”€â”€ Phase 1: Submit all â”€â”€\n",
    "print(\"ğŸ“¤ Submitting all requests...\")\n",
    "for i, img in enumerate(all_images, 1):\n",
    "    fname = os.path.basename(img)\n",
    "    for aid in ANALYZERS:\n",
    "        try:\n",
    "            op = submit(img, aid)\n",
    "            jobs.append((img, aid, op, time.time()))\n",
    "            print(f\"  âœ“ [{i}/20] {fname} â†’ {aid}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âœ— [{i}/20] {fname} â†’ {aid}: {e}\")\n",
    "            all_metrics[aid].append({\"document\": fname, \"analyzer\": aid,\n",
    "                                     \"error\": str(e), \"time_seconds\": 0})\n",
    "\n",
    "print(f\"\\nğŸ“¤ {len(jobs)} jobs submitted. Waiting 8s before polling...\")\n",
    "time.sleep(8)\n",
    "\n",
    "# â”€â”€ Phase 2: Collect results â”€â”€\n",
    "print(\"\\nğŸ“¥ Collecting results...\")\n",
    "done = 0\n",
    "for img, aid, op_url, t0 in jobs:\n",
    "    fname = os.path.basename(img)\n",
    "    dname = os.path.splitext(fname)[0]\n",
    "    done += 1\n",
    "    try:\n",
    "        res = poll(op_url)\n",
    "        dt = time.time() - t0\n",
    "        c = res.get(\"result\",{}).get(\"contents\",[])\n",
    "        block = c[0] if c else {}\n",
    "        fields = block.get(\"fields\", {})\n",
    "        md = block.get(\"markdown\", \"\")\n",
    "\n",
    "        # Extract field values (name â†’ content)\n",
    "        field_values = extract_field_values(fields)\n",
    "\n",
    "        # Count field confidences recursively\n",
    "        confs = []\n",
    "        def _gc(o):\n",
    "            if isinstance(o, dict):\n",
    "                if \"confidence\" in o: confs.append(o[\"confidence\"])\n",
    "                for v in o.values(): _gc(v)\n",
    "            elif isinstance(o, list):\n",
    "                for x in o: _gc(x)\n",
    "        _gc(fields)\n",
    "\n",
    "        # Generate global LLM description (via GPT on Cognitive Services)\n",
    "        description = llm_describe(md, fname)\n",
    "\n",
    "        m = {\"document\": fname, \"analyzer\": aid, \"time_seconds\": round(dt,1),\n",
    "             \"num_fields\": len(fields), \"markdown_len\": len(md),\n",
    "             \"avg_confidence\": round(sum(confs)/len(confs),4) if confs else None,\n",
    "             \"num_tables\": len(block.get(\"tables\",[])),\n",
    "             \"num_pages\": len(block.get(\"pages\",[])),\n",
    "             \"field_values\": field_values,\n",
    "             \"description\": description}\n",
    "        all_metrics[aid].append(m)\n",
    "\n",
    "        # Save JSON (with field values + description)\n",
    "        enriched_result = res.copy()\n",
    "        enriched_result[\"_extracted\"] = {\n",
    "            \"field_values\": field_values,\n",
    "            \"description\": description\n",
    "        }\n",
    "        folder = os.path.join(OUTPUT_FOLDER, aid)\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        with open(os.path.join(folder, f\"{dname}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(enriched_result, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        # Print summary\n",
    "        top_fields = {k: (str(v)[:60] if not isinstance(v, (list,dict)) else f\"[{type(v).__name__}]\")\n",
    "                      for k, v in list(field_values.items())[:5]}\n",
    "        print(f\"  âœ… [{done}/{len(jobs)}] {fname}|{aid} {dt:.0f}s f={len(fields)} md={len(md)}\")\n",
    "        print(f\"     ğŸ“‹ Fields: {top_fields}\")\n",
    "        print(f\"     ğŸ“ {description[:120]}\")\n",
    "    except Exception as e:\n",
    "        dt = time.time() - t0\n",
    "        print(f\"  âŒ [{done}/{len(jobs)}] {fname}|{aid} {dt:.0f}s {str(e)[:80]}\")\n",
    "        all_metrics[aid].append({\"document\": fname, \"analyzer\": aid,\n",
    "                                 \"error\": str(e), \"time_seconds\": round(dt,1)})\n",
    "\n",
    "print(f\"\\nğŸ‰ Done! Results in {OUTPUT_FOLDER}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
